{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'clip_interrogator.clip_interrogator' from '/root/miniconda3/envs/kaggle/lib/python3.8/site-packages/clip_interrogator/clip_interrogator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from clip_interrogator import clip_interrogator\n",
    "from typing import List\n",
    "\n",
    "sys.path.append('sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "import inspect\n",
    "import importlib\n",
    "from blip.models import blip\n",
    "\n",
    "# replace tokenizer path to prevent downloading\n",
    "blip_path = inspect.getfile(blip)\n",
    "\n",
    "fin = open(blip_path, \"rt\")\n",
    "data = fin.read()\n",
    "data = data.replace(\n",
    "    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n",
    "    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n",
    ")\n",
    "fin.close()\n",
    "\n",
    "fin = open(blip_path, \"wt\")\n",
    "fin.write(data)\n",
    "fin.close()\n",
    "\n",
    "# reload module\n",
    "importlib.reload(blip)\n",
    "\n",
    "clip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n",
    "\n",
    "fin = open(clip_interrogator_path, \"rt\")\n",
    "data = fin.read()\n",
    "data = data.replace(\n",
    "    'open_clip.get_tokenizer(clip_model_name)', \n",
    "    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n",
    ")\n",
    "fin.close()\n",
    "\n",
    "fin = open(clip_interrogator_path, \"wt\")\n",
    "fin.write(data)\n",
    "fin.close()\n",
    "\n",
    "importlib.reload(clip_interrogator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "class my_config:\n",
    "    device = \"cuda\"\n",
    "    comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')\n",
    "    \n",
    "    model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n",
    "    clip_model_name = \"ViT-H-14\"\n",
    "    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n",
    "    cache_path = \"/kaggle/input/clip-interrogator-models-x\"\n",
    "    \n",
    "    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n",
    "    \n",
    "    image_path = comp_path / 'images'\n",
    "    embeddings_num = 384\n",
    "    \n",
    "    st_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images list\n",
    "images = os.listdir(my_config.image_path)\n",
    "images = sorted(images)\n",
    "imgIds = [i.split('.')[0] for i in images]\n",
    "\n",
    "# imgId_eId = [\n",
    "#     '_'.join(map(str, i)) for i in zip(\n",
    "#         np.repeat(imgIds, my_config.embeddings_num),\n",
    "#         np.tile(range(my_config.embeddings_num), len(imgIds))\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "imgId_eId = []\n",
    "for image in imgIds:\n",
    "    for num in range(my_config.embeddings_num):\n",
    "        imgId_eId.append(image + \"_\" + str(num))\n",
    "print(imgId_eId[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "model_config = clip_interrogator.Config(clip_model_name=my_config.model_name)\n",
    "model_config.cache_path = my_config.cache_path\n",
    "\n",
    "# load clip model\n",
    "clip_model = open_clip.create_model(my_config.clip_model_name, precision='fp16' if model_config.device == \"cuda\" else 'fp32')\n",
    "open_clip.load_checkpoint(clip_model, my_config.clip_model_path)\n",
    "clip_model.to(model_config.device).eval()\n",
    "model_config.clip_model = clip_model\n",
    "\n",
    "clip_preprocess = open_clip.image_transform(\n",
    "    clip_model.visual.image_size,\n",
    "    is_train = False,\n",
    "    mean = getattr(clip_model.visual, 'image_mean', None),\n",
    "    std = getattr(clip_model.visual, 'image_std', None)\n",
    ")\n",
    "model_config.clip_preprocess = clip_preprocess\n",
    "\n",
    "# load blip model\n",
    "configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\n",
    "med_config = os.path.join(configs_path, 'med_config.json')\n",
    "\n",
    "blip_model = blip.blip_decoder(\n",
    "    pretrained = my_config.blip_model_path,\n",
    "    image_size = model_config.blip_image_eval_size, \n",
    "    vit = model_config.blip_model_type, \n",
    "    med_config = med_config\n",
    ")\n",
    "\n",
    "blip_model.eval()\n",
    "blip_model = blip_model.to(model_config.device)\n",
    "model_config.blip_model = blip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = clip_interrogator.Interrogator(model_config)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "medium_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\n",
    "movement_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\n",
    "flaves_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_array, merged_labels = [], []\n",
    "for i in range(len(medium_features_array)):\n",
    "    merged_array.append(medium_features_array[i])\n",
    "    merged_labels.append(ci.mediums.labels[i])\n",
    "for i in range(len(movement_features_array)):\n",
    "    merged_array.append(movement_features_array[i])\n",
    "    merged_labels.append(ci.movements.labels[i])\n",
    "for i in range(len(flaves_features_array)):\n",
    "    merged_array.append(flaves_features_array[i])\n",
    "    merged_labels.append(ci.flavors.labels[i])\n",
    "    \n",
    "merged_array = torch.stack(merged_array)\n",
    "\n",
    "print(len(merged_array), len(merged_labels))\n",
    "print(merged_array[0])\n",
    "print(merged_labels[0])\n",
    "print(medium_features_array[0])\n",
    "print(ci.mediums.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_interrogate_classic(image: Image, image_features: torch.Tensor, caption: str) -> str:\n",
    "#     print(image)\n",
    "#     caption = ci.generate_caption(image)\n",
    "    \n",
    "#     image_features = ci.image_to_features(image)\n",
    "\n",
    "    medium = [ci.mediums.labels[i] for i in cos(image_features, medium_features_array).topk(1).indices][0]\n",
    "    movement = [ci.movements.labels[i] for i in cos(image_features, movement_features_array).topk(2).indices][0]\n",
    "    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flaves_features_array).topk(3).indices])\n",
    "\n",
    "    if caption.startswith(medium):\n",
    "        prompt = f\"{caption}, {movement}, {flaves}\"\n",
    "    else:\n",
    "        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n",
    "\n",
    "    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_interrogate_fast(image: Image, image_features: torch.Tensor, caption: str):\n",
    "#     caption = ci.generate_caption(image)\n",
    "#     image_features = ci.image_to_features(image)\n",
    "    \n",
    "    merged_ans = [merged_labels[i] for i in cos(image_features, merged_array).topk(10).indices]\n",
    "    return str(clip_interrogator._truncate_to_fit(caption + \", \" + \", \".join(merged_ans), ci.tokenize)), merged_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_interrogate(image: Image) -> str:\n",
    "    caption = ci.generate_caption(image)\n",
    "    image_features = ci.image_to_features(image)\n",
    "    \n",
    "    fast_prompt, flaves = my_interrogate_fast(image, image_features, caption)\n",
    "#     flaves = [merged_labels[i] for i in cos(image_features, merged_array).topk(16).indices]\n",
    "    \n",
    "#     best_prompt, best_sim = caption, ci.similarity(image_features, caption)\n",
    "#     best_prompt = ci.chain(image_features, flaves, best_prompt, best_sim, min_count=2, max_count=8, desc=\"Flavor chain\")\n",
    "    \n",
    "    classic_prompt = my_interrogate_classic(image, image_features, caption)\n",
    "#     candidates = [caption, classic_prompt, fast_prompt, best_prompt]\n",
    "    candidates = [caption, classic_prompt, fast_prompt]\n",
    "    return candidates[np.argmax(ci.similarities(image_features, candidates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator prompts\n",
    "prompts = []\n",
    "\n",
    "image_path = str(my_config.image_path) + \"/\"\n",
    "print(image_path)\n",
    "print(model_config.device)\n",
    "\n",
    "for image in images:\n",
    "    tmp_path = image_path + image\n",
    "#     print(tmp_path)\n",
    "    \n",
    "    img = Image.open(tmp_path).convert(\"RGB\")\n",
    "    \n",
    "#     img_ans = my_interrogate_classic(img)\n",
    "#     print(img_ans)\n",
    "#     img_ans = my_interrogate_fast(img)\n",
    "#     print(img_ans)\n",
    "#     img_ans, _img_ans = my_interrogate_fast(img)\n",
    "    img_ans = my_interrogate(img)\n",
    "#     print(img_ans)\n",
    "    \n",
    "    prompts.append(img_ans)\n",
    "\n",
    "print(\"prompts done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal the prompt embeddings\n",
    "\n",
    "# prompt_embeddings = []\n",
    "# for i in range(len(imgIds) * 384):\n",
    "#     prompt_embeddings.append(2 * np.random.random() - 1)\n",
    "print(prompts[0])\n",
    "\n",
    "st_model = SentenceTransformer(my_config.st_model_path)\n",
    "\n",
    "prompt_embeddings = st_model.encode(prompts).flatten()\n",
    "# image_embeddings = st_model.encode(Image.open(str(my_config.image_path) + \"/\" + \"20057f34d.png\").convert(\"RGB\")).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator the result csv\n",
    "print(len(imgId_eId), len(prompt_embeddings))\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    index = imgId_eId,\n",
    "    data = prompt_embeddings,\n",
    "    columns = ['val']\n",
    ").rename_axis('imgId_eId')\n",
    "\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
